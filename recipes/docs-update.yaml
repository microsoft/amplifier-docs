# =============================================================================
# Documentation Update - Combined Sync + Regeneration Recipe
# =============================================================================
#
# Chains docs-sync and docs-regenerate into a single "run docs-update" workflow:
#
#   SYNC PHASE:    Clone/pull ~34 source repos, compute source hashes, diff
#                  against baseline to detect what changed.
#
#   GUARD:         If mode=incremental and no changes detected, stop early
#                  with "docs are up to date" (no approval needed).
#
#   REGEN PHASE:   Select changed docs, LLM surgical regeneration, evidence
#                  validation, semantic diff filter (<5% = cosmetic, rejected),
#                  apply accepted files to docs/.
#
#   REVIEW:        Approval gate - human reviews git diff before commit.
#
#   COMMIT PHASE:  Update per-section baseline, commit docs + baseline together.
#
# The standalone recipes (docs-sync.yaml, docs-regenerate.yaml) remain
# available for individual use.
#
# Usage:
#   amplifier tool invoke recipes operation=execute \
#     recipe_path=recipes/docs-update.yaml
#
#   # Override context:
#   amplifier tool invoke recipes operation=execute \
#     recipe_path=recipes/docs-update.yaml \
#     context='{"mode": "full", "batch_limit": 5}'
#
# =============================================================================

name: "docs-update"
description: "Combined sync + regeneration workflow: sync sources, detect changes, regenerate if needed, with approval-gated commit"
version: "1.0.0"
tags: ["documentation", "sync", "regeneration", "incremental", "combined"]

context:
  # Mode: "incremental" (only changed sources), "full" (all sections), "single" (one doc)
  mode: "incremental"
  # For single mode: specific doc path to regenerate
  doc_path: ""
  # Limit number of docs to regenerate per run
  batch_limit: 10
  # Source directories
  repos_dir: "~/repo/amplifier-sources"
  docs_dir: "./docs"
  outline_path: "./outlines/amplifier-docs-outline.json"
  # Hash files for incremental detection
  hashes_path: "./sync-output/cache/source-hashes.json"
  baseline_path: "./baseline-hashes.json"
  # Output
  output_dir: "./sync-output/regenerated"
  # Minimum new content ratio to accept changes (0.0-1.0)
  # Files with less than this ratio of new content are considered "cosmetic only" and skipped
  min_change_ratio: "0.05"

stages:
  # ===========================================================================
  # STAGE 1: Sync sources, detect changes, regenerate, validate, apply, review
  # ===========================================================================
  - name: "sync-and-regenerate"
    steps:

      # -----------------------------------------------------------------------
      # SYNC PHASE: Clone/pull repos and compute source hashes
      # -----------------------------------------------------------------------

      - id: "setup"
        type: "bash"
        command: |
          mkdir -p "$(dirname '{{hashes_path}}')" "{{output_dir}}/staging"
          echo 'Setup complete. Mode: {{mode}}'
        output: "setup_result"
        timeout: 30

      - id: "generate-outline"
        type: "bash"
        description: "Regenerate outline JSON from DOC_SOURCE_MAPPING.csv (always fresh)"
        command: |
          uv run python scripts/csv_to_outline.py
          echo "Outline regenerated from docs/DOC_SOURCE_MAPPING.csv"
        output: "outline_result"
        timeout: 30

      - id: "extract-repos"
        type: "bash"
        command: "python3 -c \"import json; d=json.load(open('{{outline_path}}')); repos=set(); [repos.update(v) for v in d.get('_meta',{}).get('allowed_repos',{}).values()]; print(json.dumps(sorted(list(repos))))\""
        output: "repo_list"
        parse_json: true
        timeout: 30

      - id: "sync-repos"
        agent: "foundation:git-ops"
        prompt: |
          Clone or update the following repositories to the directory: {{repos_dir}}

          Repositories to sync (from microsoft GitHub org):
          {{repo_list}}

          For each repository:
          1. If it exists in {{repos_dir}}, do a git pull to update
          2. If it doesn't exist, clone it with: git clone --depth 1 https://github.com/microsoft/{repo_name}

          Work through all repositories. Report how many succeeded, failed, or were skipped.
          This may take several minutes as there are ~34 repositories.
        output: "sync_result"
        timeout: 1200

      - id: "drift-check"
        type: "bash"
        command: |
          python3 << 'EOF'
          import json
          import re
          import sys
          from pathlib import Path

          repos_dir = Path("{{repos_dir}}").expanduser()
          outline_path = Path("{{outline_path}}")

          # 1. Read MODULES.md from the synced amplifier repo
          modules_md_path = repos_dir / "amplifier" / "docs" / "MODULES.md"
          if not modules_md_path.exists():
              print(json.dumps({
                  "status": "skipped",
                  "reason": f"MODULES.md not found at {modules_md_path}",
                  "stale": [],
                  "missing": []
              }))
              sys.exit(0)

          modules_text = modules_md_path.read_text()

          # 2. Extract Microsoft repo names from github.com/microsoft/amplifier-* URLs
          #    This excludes community repos (robotdad/, colombod/, etc.)
          modules_repos = set()
          for match in re.finditer(r'github\.com/microsoft/(amplifier-[a-zA-Z0-9_-]+)', modules_text):
              repo_name = match.group(1)
              modules_repos.add(repo_name)

          # 3. Read the outline JSON
          with open(outline_path) as f:
              outline = json.load(f)

          # 4. Extract the pipeline's repo list from _meta.allowed_repos
          pipeline_repos = set()
          for repo_list in outline.get("_meta", {}).get("allowed_repos", {}).values():
              pipeline_repos.update(repo_list)

          # 5. Compute drift
          #    Exclude 'amplifier' itself â€” it's the entry-point repo, not a documented component
          pipeline_for_comparison = pipeline_repos - {"amplifier"}
          modules_for_comparison = modules_repos - {"amplifier"}

          stale = sorted(pipeline_for_comparison - modules_for_comparison)
          missing = sorted(modules_for_comparison - pipeline_for_comparison)

          result = {
              "status": "checked",
              "modules_md_repos": len(modules_repos),
              "pipeline_repos": len(pipeline_repos),
              "stale": stale,
              "missing": missing
          }
          print(json.dumps(result))

          # 6. Print human-readable warnings to stderr (don't fail the pipeline)
          if stale:
              print(f"\nWARNING: {len(stale)} repo(s) in pipeline but NOT in MODULES.md (potential dead repos):", file=sys.stderr)
              for r in stale:
                  print(f"  - {r}", file=sys.stderr)

          if missing:
              print(f"\nINFO: {len(missing)} repo(s) in MODULES.md but NOT in pipeline (potential coverage gaps):", file=sys.stderr)
              for r in missing:
                  print(f"  - {r}", file=sys.stderr)

          if not stale and not missing:
              print("Drift check: pipeline and MODULES.md are in sync.", file=sys.stderr)
          EOF
        output: "drift_check_result"
        parse_json: true
        timeout: 60

      - id: "compute-hashes"
        agent: "foundation:file-ops"
        prompt: |
          Compute SHA256 hashes of source files referenced in the documentation outline.

          1. Read the outline from: {{outline_path}}
          2. For each section in content_sections, check if its source files exist in {{repos_dir}}
          3. For files that exist, compute their SHA256 hash (first 16 chars is enough)
          4. Track which sources are found vs missing

          Save the results as JSON to: {{hashes_path}}

          The JSON should have this structure:
          {
            "summary": {"total_sources": N, "found": N, "missing": N},
            "sections": {
              "section-id": {
                "doc_path": "docs/...",
                "sources": [{"repo": "...", "path": "...", "exists": true/false, "hash": "..."}]
              }
            }
          }

          Print a summary of found vs missing sources.
        output: "hash_result"
        timeout: 300

      - id: "hash-diff"
        type: "bash"
        command: |
          python3 << 'EOF'
          import json
          from pathlib import Path

          hashes_path = "{{hashes_path}}"
          baseline_path = "{{baseline_path}}"

          with open(hashes_path) as f:
              current = json.load(f)

          try:
              with open(baseline_path) as f:
                  baseline = json.load(f)
          except FileNotFoundError:
              baseline = {"sections": {}}

          no_baseline = len(baseline.get("sections", {})) == 0
          changed = []
          unchanged = []
          missing_sources = []

          for section_id, section in current.get("sections", {}).items():
              dp = section.get("doc_path", "")
              cur_hashes = {(s.get("repo",""), s.get("path","")): s.get("hash") for s in section.get("sources", [])}
              base_hashes = {(s.get("repo",""), s.get("path","")): s.get("hash") for s in baseline.get("sections", {}).get(section_id, {}).get("sources", [])}

              missing = [s for s in section.get("sources", []) if not s.get("exists", True)]
              if missing:
                  missing_sources.append({"section": section_id, "doc_path": dp, "missing": [f"{s['repo']}/{s['path']}" for s in missing]})

              if cur_hashes != base_hashes:
                  changed_files = [k for k in cur_hashes if cur_hashes[k] != base_hashes.get(k)]
                  new_files = [k for k in cur_hashes if k not in base_hashes]
                  changed.append({"section": section_id, "doc_path": dp, "changed_files": len(changed_files), "new_files": len(new_files)})
              else:
                  unchanged.append(section_id)

          result = {
              "no_baseline": no_baseline,
              "total_sections": len(current.get("sections", {})),
              "changed": len(changed),
              "unchanged": len(unchanged),
              "missing_sources": len(missing_sources),
              "changed_sections": changed,
              "missing_source_details": missing_sources
          }
          print(json.dumps(result))
          EOF
        output: "hash_diff_result"
        parse_json: true
        timeout: 60

      # Guard step: stop early if no changes detected.
      # In incremental mode with no changes, exits non-zero which triggers
      # on_error: skip_remaining -> recipe ends as partial success with
      # "docs are up to date" message. No approval gate, no user interaction.
      # Full and single modes always continue (select-docs handles filtering).
      - id: "check-changes"
        type: "bash"
        command: |
          python3 << 'EOF'
          import json, sys

          mode = "{{mode}}"
          diff = json.loads('''{{hash_diff_result}}''')
          changed = diff["changed"]
          no_baseline = diff["no_baseline"]
          total = diff["total_sections"]

          # Full mode always continues (regenerate everything regardless of changes)
          if mode == "full":
              print(f"Full mode: will regenerate all {total} sections (batch_limit applies)")
              sys.exit(0)

          # Single mode always continues (select-docs handles the filtering)
          if mode == "single":
              doc_path = "{{doc_path}}"
              print(f"Single mode: targeting '{doc_path}'")
              sys.exit(0)

          # Incremental mode: stop early if nothing changed
          if changed == 0 and not no_baseline:
              print("=" * 60)
              print("DOCS ARE UP TO DATE")
              print("=" * 60)
              print(f"\nTotal sections checked: {total}")
              print("No source files have changed since last regeneration.")
              print("Nothing to regenerate. Exiting early.")
              sys.exit(1)  # Triggers on_error: skip_remaining

          # Changes found - continue to regeneration
          reason = "first run (no baseline)" if no_baseline else f"{changed} section(s) have changed sources"
          print(f"Changes detected: {reason}")
          print(f"Sections: {total} total, {changed} changed, {diff['unchanged']} unchanged")
          if diff.get("changed_sections"):
              print("\nChanged sections:")
              for s in diff["changed_sections"]:
                  print(f"  - {s['doc_path']} ({s['changed_files']} files changed)")
          EOF
        output: "change_check"
        on_error: "skip_remaining"
        timeout: 30

      # -----------------------------------------------------------------------
      # REGENERATION PHASE: Select, regenerate, validate, filter, apply
      # -----------------------------------------------------------------------

      # Select docs needing regeneration based on mode + hash comparison
      - id: "select-docs"
        type: "bash"
        command: |
          python3 << 'EOF'
          import json, sys

          mode = "{{mode}}"
          doc_path = "{{doc_path}}"
          batch_limit = int("{{batch_limit}}")

          try:
              with open("{{hashes_path}}") as f:
                  current = json.load(f)
          except FileNotFoundError:
              print('{"error": "No source-hashes.json found. Run docs-sync first.", "count": 0, "docs": []}')
              sys.exit(0)

          with open("{{outline_path}}") as f:
              outline = json.load(f)

          try:
              with open("{{baseline_path}}") as f:
                  baseline = json.load(f)
          except FileNotFoundError:
              baseline = {"sections": {}}

          section_lookup = {s["doc_path"]: s for s in outline.get("content_sections", [])}

          def make_staging_name(dp):
              """e.g. 'docs/architecture/index.md' -> 'docs__architecture__index.md'"""
              return dp.replace("/", "__")

          def section_changed(section_id):
              """Compare source hashes: any difference means section needs regeneration."""
              cur = current.get("sections", {}).get(section_id, {})
              base = baseline.get("sections", {}).get(section_id, {})
              cur_hashes = {(s.get("repo",""), s.get("path","")): s.get("hash") for s in cur.get("sources", [])}
              base_hashes = {(s.get("repo",""), s.get("path","")): s.get("hash") for s in base.get("sources", [])}
              return cur_hashes != base_hashes

          no_baseline = len(baseline.get("sections", {})) == 0
          selected = []

          for section_id, section in current.get("sections", {}).items():
              dp = section.get("doc_path", "")
              if not dp:
                  continue

              if mode == "single":
                  if doc_path and dp != doc_path:
                      continue
              elif mode == "incremental":
                  if not section_changed(section_id):
                      continue
              # Full mode: process everything (no filtering)

              outline_section = section_lookup.get(dp, {})
              if not outline_section.get("sources"):
                  continue

              selected.append({
                  "doc_path": dp,
                  "staging_name": make_staging_name(dp),
                  "sources": outline_section.get("sources", []),
              })

              if len(selected) >= batch_limit:
                  break

          result = {
              "count": len(selected),
              "mode": mode,
              "no_baseline": no_baseline,
              "total_sections": len(current.get("sections", {})),
              "docs": selected
          }
          print(json.dumps(result))
          EOF
        output: "selected_docs"
        parse_json: true
        timeout: 60

      # Read existing doc content for surgical comparison
      - id: "read-existing-docs"
        type: "bash"
        command: |
          python3 << 'EOF'
          import json
          import os
          from pathlib import Path

          selected = json.loads('''{{selected_docs}}''')
          docs_dir = "{{docs_dir}}"
          output_dir = "{{output_dir}}"

          existing = {}

          for doc in selected.get("docs", []):
              doc_path = doc["doc_path"]
              full_path = Path(docs_dir).parent / doc_path  # doc_path includes "docs/"

              if full_path.exists():
                  with open(full_path) as f:
                      content = f.read()

                  frontmatter = ""
                  body = content
                  if content.startswith("---"):
                      parts = content.split("---", 2)
                      if len(parts) >= 3:
                          frontmatter = parts[1].strip()
                          body = parts[2].strip()

                  existing[doc_path] = {
                      "full_content": content,
                      "frontmatter": frontmatter,
                      "body": body,
                      "line_count": len(content.splitlines())
                  }
              else:
                  existing[doc_path] = {"exists": False}

          staging_dir = Path(output_dir) / "staging"
          with open(staging_dir / "existing-docs.json", "w") as f:
              json.dump(existing, f, indent=2)

          print(json.dumps({
              "docs_read": len([d for d in existing.values() if d.get("full_content")]),
              "docs_missing": len([d for d in existing.values() if not d.get("full_content")])
          }))
          EOF
        output: "existing_docs_result"
        parse_json: true
        timeout: 60

      # Surgical LLM regeneration: preserve unchanged content, update only diffs
      - id: "regenerate-docs"
        agent: "foundation:zen-architect"
        mode: "ARCHITECT"
        prompt: |
          Perform SURGICAL documentation updates. You MUST preserve existing content that doesn't need changes.

          ## CRITICAL: DETERMINISM REQUIREMENTS

          1. **PRESERVE unchanged sections VERBATIM** - Copy them exactly, character-for-character
          2. **Use EXACT terminology from source code** - Don't paraphrase; use the same words
          3. **Keep IDENTICAL structure** - Same heading hierarchy, same section order
          4. **MINIMAL changes only** - Only update what actually differs between source and doc
          5. **Match EXACT formatting** - Same code fence style, same list format, same spacing

          ## Existing Documentation

          The current docs have been saved to: {{output_dir}}/staging/existing-docs.json

          Read this file FIRST to see the current content for each doc.

          ## Selected Docs to Update
          {{selected_docs}}

          These docs were selected because their source files changed since the last regeneration.

          ## Source Repository Location
          All source repos are in: {{repos_dir}}

          ## For EACH doc:

          1. **Read the existing doc** from existing-docs.json
             - Note the exact frontmatter (preserve it exactly)
             - Note the structure and section headings
             - Identify which sections DON'T need changes (keep verbatim)

          2. **Read ALL source files** listed in "sources" from {{repos_dir}}/{repo}/{path}

          3. **Compare source code against existing doc** to find gaps:
             - Config keys/options in source but missing from doc
             - API signatures that changed (parameters, return types, defaults)
             - New features or capabilities added to source
             - Model names or versions that are outdated in doc
             - Environment variables documented in source but missing from doc

          4. **Surgically update** the document:
             - START with the existing doc content (copy it)
             - KEEP all unchanged sections exactly as they are
             - INSERT new content in the appropriate location
             - UPDATE only the specific values/lists that differ from source
             - PRESERVE the exact frontmatter

          5. **Save to**: {{output_dir}}/staging/{staging_name}
             - Where staging_name is provided in each doc's entry (e.g. "docs__architecture__index.md")
             - This avoids filename collisions when multiple docs share the same basename

          ## What NOT to do:
          - Do NOT rephrase existing sentences that are still accurate
          - Do NOT reorganize sections that don't need it
          - Do NOT change formatting style (code fence language, list markers)
          - Do NOT add content that isn't in the source files

          Work through ALL selected docs. Be surgical - the goal is MINIMAL diff.
        output: "regeneration_result"
        timeout: 900

      # Evidence-based validation: verify all claims against source code
      - id: "evidence-validation"
        agent: "foundation:zen-architect"
        mode: "REVIEW"
        prompt: |
          You are a SKEPTICAL EVIDENCE REVIEWER. Your job is to verify that every
          factual claim in the regenerated docs is grounded in actual source code.

          ## Regenerated docs
          Read .md files from: {{output_dir}}/staging/ (ignore existing-docs.json)
          Files use collision-safe names (e.g. "docs__architecture__index.md").
          Match them to their doc_path using the staging_name field in the selected docs below.

          ## Source repos
          All source repos are at: {{repos_dir}}

          ## Doc-to-source mappings
          {{selected_docs}}

          ## Your process for EACH regenerated doc:

          1. Read the regenerated .md file from staging
          2. Read its corresponding source files from {{repos_dir}}/{repo}/{path}
          3. Extract every factual claim in the doc:
             - Config keys and their types/defaults
             - API signatures (function names, parameters, return types)
             - Feature descriptions and capabilities
             - Model names and versions
             - Environment variables
             - Default values and constants
             - CLI commands and flags
          4. For EACH claim, search the source code for evidence:
             - VERIFIED: Claim matches source. Keep it.
             - NOT FOUND: Claim has no source evidence. REMOVE it from the doc.
             - CONTRADICTED: Source says something different. FIX the doc to match source.
             - OUTDATED: Value changed in source. UPDATE the doc with current value.
          5. Save the corrected doc back to {{output_dir}}/staging/ (overwrite)

          ## Output

          After processing all docs, print a JSON validation report:
          {
            "docs_validated": N,
            "total_claims": N,
            "verified": N,
            "fixed": N,
            "removed": N,
            "details": [
              {
                "doc": "filename.md",
                "claims_checked": N,
                "verified": N,
                "fixes": [{"claim": "...", "was": "...", "now": "...", "source_file": "..."}],
                "removals": [{"claim": "...", "reason": "no evidence in source"}]
              }
            ]
          }

          ## RULES
          - If you CANNOT find evidence for a claim in the source files, REMOVE IT
          - Do NOT invent features, config keys, or behaviors
          - When fixing a value, use the EXACT value from source code
          - Preserve all document structure, headings, and frontmatter
          - Only modify factual content, not prose style or formatting
          - Be thorough: check every config table, every feature list, every code example
        output: "evidence_result"
        parse_json: true
        timeout: 900

      # Semantic diff: reject changes below 5% new word-set content as cosmetic
      - id: "semantic-diff-filter"
        type: "bash"
        command: |
          python3 << 'EOF'
          import json
          import re
          import shutil
          from pathlib import Path

          output_dir = Path("{{output_dir}}")
          docs_dir = Path("{{docs_dir}}").parent  # Go up since doc_path includes "docs/"
          staging_dir = output_dir / "staging"
          min_ratio = float("{{min_change_ratio}}")

          selected = json.loads('''{{selected_docs}}''')
          staging_to_docpath = {}
          for doc in selected.get("docs", []):
              staging_to_docpath[doc["staging_name"]] = doc["doc_path"]

          existing_path = staging_dir / "existing-docs.json"
          if existing_path.exists():
              with open(existing_path) as f:
                  existing = json.load(f)
          else:
              existing = {}

          results = {"accepted": [], "rejected": [], "analysis": []}

          def extract_meaningful_content(text):
              """Extract words, ignoring formatting and common words."""
              text = re.sub(r'^---\n.*?\n---\n', '', text, flags=re.DOTALL)
              text = re.sub(r'```\w*\n', '', text)
              text = re.sub(r'```', '', text)
              words = re.findall(r'\b[a-zA-Z_][a-zA-Z0-9_]*\b', text.lower())
              stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',
                           'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',
                           'would', 'could', 'should', 'may', 'might', 'must', 'shall',
                           'can', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by',
                           'from', 'or', 'and', 'not', 'this', 'that', 'it', 'as', 'if'}
              return [w for w in words if w not in stopwords and len(w) > 2]

          for staged_file in staging_dir.glob("*.md"):
              staging_name = staged_file.name

              doc_path_str = staging_to_docpath.get(staging_name)
              if not doc_path_str:
                  results["rejected"].append({"file": staging_name, "reason": "no mapping found"})
                  continue

              original_path = docs_dir / doc_path_str

              if not original_path.exists():
                  # New file - always accept
                  shutil.copy(staged_file, output_dir / staging_name)
                  results["accepted"].append({"file": staging_name, "doc_path": doc_path_str, "reason": "new file"})
                  continue

              with open(original_path) as f:
                  original_content = f.read()
              with open(staged_file) as f:
                  staged_content = f.read()

              if original_content.strip() == staged_content.strip():
                  results["rejected"].append({"file": staging_name, "doc_path": doc_path_str, "reason": "identical"})
                  continue

              orig_words = set(extract_meaningful_content(original_content))
              staged_words = set(extract_meaningful_content(staged_content))

              new_words = staged_words - orig_words
              removed_words = orig_words - staged_words

              total_original = len(orig_words) if orig_words else 1
              new_ratio = len(new_words) / total_original

              analysis = {
                  "file": staging_name,
                  "doc_path": doc_path_str,
                  "original_words": len(orig_words),
                  "staged_words": len(staged_words),
                  "new_words": len(new_words),
                  "removed_words": len(removed_words),
                  "new_ratio": round(new_ratio, 4),
                  "threshold": min_ratio,
                  "new_word_sample": list(new_words)[:15]
              }

              if new_ratio >= min_ratio:
                  shutil.copy(staged_file, output_dir / staging_name)
                  analysis["status"] = "ACCEPTED"
                  results["accepted"].append({"file": staging_name, "doc_path": doc_path_str, "new_ratio": new_ratio, "new_words": len(new_words)})
              else:
                  analysis["status"] = "REJECTED (cosmetic)"
                  results["rejected"].append({"file": staging_name, "doc_path": doc_path_str, "new_ratio": new_ratio, "reason": "below threshold"})

              results["analysis"].append(analysis)

          with open(output_dir / "diff-analysis.json", "w") as f:
              json.dump(results, f, indent=2)

          print(json.dumps({
              "accepted": len(results["accepted"]),
              "rejected": len(results["rejected"]),
              "accepted_files": [a["file"] for a in results["accepted"]],
              "rejected_files": [r["file"] for r in results["rejected"]]
          }))
          EOF
        output: "diff_filter_result"
        parse_json: true
        timeout: 120

      # Validate markdown structure of accepted files
      - id: "validate-output"
        agent: "foundation:file-ops"
        prompt: |
          Validate the accepted regenerated documentation files.

          Diff filter results:
          {{diff_filter_result}}

          1. List files directly in: {{output_dir}} (not staging subdirectory)
             Only .md files that passed the semantic diff filter

          2. For each file, check:
             - Has valid markdown frontmatter (starts with ---)
             - Has proper heading structure
             - No obvious placeholder text

          3. Report validation summary.
        output: "validation_result"
        timeout: 120

      # Copy accepted regenerated files to their correct doc locations
      - id: "apply-to-docs"
        type: "bash"
        command: |
          python3 << 'EOF'
          import json
          import shutil
          from pathlib import Path

          output_dir = Path("{{output_dir}}")

          selected = json.loads('''{{selected_docs}}''')
          staging_to_docpath = {}
          for doc in selected.get("docs", []):
              staging_to_docpath[doc["staging_name"]] = doc["doc_path"]

          applied = []
          skipped = []

          for md_file in sorted(output_dir.glob("*.md")):
              staging_name = md_file.name
              if staging_name in staging_to_docpath:
                  doc_path = staging_to_docpath[staging_name]
                  target = Path(".") / doc_path
                  target.parent.mkdir(parents=True, exist_ok=True)
                  shutil.copy(md_file, target)
                  applied.append({"file": staging_name, "doc_path": doc_path, "target": str(target)})
              else:
                  skipped.append(staging_name)

          print(json.dumps({
              "applied": len(applied),
              "skipped": len(skipped),
              "files": applied,
              "skipped_files": skipped
          }))
          EOF
        output: "apply_result"
        parse_json: true
        timeout: 60

      # Show summary and git diff for human review
      - id: "show-diff"
        type: "bash"
        command: |
          python3 << 'PYEOF'
          import json

          print("=== DOCS-UPDATE SUMMARY ===\n")

          # Hash diff
          print("Source change detection:")
          try:
              hd = json.loads('''{{hash_diff_result}}''')
              print(f"  Total sections: {hd.get('total_sections', 'N/A')}")
              print(f"  Changed: {hd.get('changed', 'N/A')}")
              print(f"  Unchanged: {hd.get('unchanged', 'N/A')}")
              if hd.get('no_baseline'):
                  print("  (First run - no baseline, all sections treated as changed)")
          except Exception:
              print("  (see hash_diff_result for details)")

          # Evidence validation
          print("\nEvidence validation:")
          try:
              ev = json.loads('''{{evidence_result}}''')
              print(f"  Claims checked: {ev.get('total_claims', 'N/A')}")
              print(f"  Verified: {ev.get('verified', 'N/A')}")
              print(f"  Fixed: {ev.get('fixed', 'N/A')}")
              print(f"  Removed: {ev.get('removed', 'N/A')}")
              for detail in ev.get("details", []):
                  if detail.get("fixes") or detail.get("removals"):
                      print(f"  {detail['doc']}:")
                      for fix in detail.get("fixes", []):
                          print(f"    FIXED: {fix['claim'][:60]}...")
                      for rm in detail.get("removals", []):
                          print(f"    REMOVED: {rm['claim'][:60]}...")
          except Exception:
              print("  (see evidence_result for details)")

          # Diff filter
          print("\nSemantic diff filter:")
          try:
              df = json.loads('''{{diff_filter_result}}''')
              print(f"  Accepted: {df.get('accepted', 0)} files")
              print(f"  Rejected: {df.get('rejected', 0)} files (cosmetic only)")
          except Exception:
              print("  (see diff_filter_result for details)")

          # Drift check
          try:
              dc = json.loads('''{{drift_check_result}}''')
              if dc.get("status") == "checked" and (dc.get("stale") or dc.get("missing")):
                  print("\nDrift check:")
                  print(f"  MODULES.md repos: {dc.get('modules_md_repos', 'N/A')}")
                  print(f"  Pipeline repos: {dc.get('pipeline_repos', 'N/A')}")
                  if dc.get("stale"):
                      print(f"  WARNING - Stale repos (in pipeline, not in MODULES.md):")
                      for r in dc["stale"]:
                          print(f"    - {r}")
                  if dc.get("missing"):
                      print(f"  INFO - Missing repos (in MODULES.md, not in pipeline):")
                      for r in dc["missing"]:
                          print(f"    - {r}")
          except Exception:
              pass

          # Applied files
          print("\nApplied to docs/:")
          try:
              ar = json.loads('''{{apply_result}}''')
              if ar.get("files"):
                  for f in ar["files"]:
                      print(f"  {f.get('doc_path', f.get('file','?'))}")
              else:
                  print("  (no files applied)")
          except Exception:
              print("  (see apply_result for details)")
          PYEOF

          echo ""
          echo "=== GIT DIFF ==="
          git diff --stat
          echo ""
          git diff
        output: "diff_output"
        timeout: 60

    # Approval gate: review the diff before committing
    approval:
      required: true
      prompt: |
        Documentation synced, regenerated, evidence-validated, and applied.
        Review the diff above. Approve to commit, deny to discard.
      timeout: 0
      default: "deny"

  # ===========================================================================
  # STAGE 2: Update baseline + commit (runs only after approval)
  # ===========================================================================
  - name: "commit"
    steps:

      # Per-section baseline update: only advance baseline for applied docs
      # Runs BEFORE commit so baseline-hashes.json is included in the commit
      - id: "update-baseline"
        type: "bash"
        command: |
          python3 << 'EOF'
          import json
          from pathlib import Path

          applied = json.loads('''{{apply_result}}''')
          committed_docs = {f["doc_path"] for f in applied.get("files", [])}

          if not committed_docs:
              print(json.dumps({"sections_updated": 0, "message": "No docs applied, baseline unchanged"}))
              exit(0)

          with open("{{hashes_path}}") as f:
              current = json.load(f)

          baseline_path = Path("{{baseline_path}}")
          if baseline_path.exists():
              with open(baseline_path) as f:
                  baseline = json.load(f)
          else:
              baseline = {"sections": {}}

          # Merge: only update sections whose docs were actually applied
          updated = 0
          for section_id, section_data in current.get("sections", {}).items():
              if section_data.get("doc_path") in committed_docs:
                  baseline["sections"][section_id] = section_data
                  updated += 1

          baseline["last_updated"] = current.get("generated_at", "")

          with open(baseline_path, "w") as f:
              json.dump(baseline, f, indent=2)

          print(json.dumps({"sections_updated": updated, "total_baseline": len(baseline["sections"])}))
          EOF
        output: "baseline_result"
        parse_json: true
        timeout: 60

      # Commit docs + updated baseline together
      - id: "commit-changes"
        agent: "foundation:git-ops"
        prompt: |
          Commit the documentation changes that were regenerated and approved.

          Applied files: {{apply_result}}
          Evidence validation results: {{evidence_result}}
          Diff filter results: {{diff_filter_result}}
          Updated baseline: {{baseline_result}}

          Stage ALL modified files including:
          - The regenerated doc files (already in working tree from the apply step)
          - {{baseline_path}} (just updated with new hashes for committed sections)

          Create a commit with:
          - Type prefix "docs:"
          - Summarize which docs were regenerated
          - Note evidence validation results (claims checked/fixed/removed)
          - Keep the message concise
        output: "commit_result"
        timeout: 120

      # Final report
      - id: "generate-report"
        type: "bash"
        command: |
          echo "=== DOCS-UPDATE COMPLETE ==="
          echo ""
          echo "Commit: {{commit_result}}"
          echo ""
          echo "Baseline: {{baseline_result}}"
          echo ""
          if [ -f "{{output_dir}}/diff-analysis.json" ]; then
            echo "Change details:"
            python3 -c "
          import json
          with open('{{output_dir}}/diff-analysis.json') as f:
              d = json.load(f)
          for a in d.get('analysis', []):
              status = 'ACCEPTED' if a.get('status') == 'ACCEPTED' else 'SKIPPED'
              print(f'  [{status}] {a[\"file\"]}: {a[\"new_words\"]} new words ({a[\"new_ratio\"]*100:.1f}% change)')
          "
          fi
        output: "final_report"
        timeout: 30
